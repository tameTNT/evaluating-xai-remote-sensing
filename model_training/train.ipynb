{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert to python script after running top to bottom in Jupyter without interactions.",
   "id": "77dd7517f5925b3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:25.781827Z",
     "start_time": "2025-03-03T12:59:25.774617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# for when on NCC to be able to import local packages\n",
    "os.chdir(os.path.expanduser(\"~/l3_project\"))"
   ],
   "id": "348d99628124a29f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "57d01b96a727e4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:17:40.642763Z",
     "start_time": "2025-03-03T13:17:40.630082Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import platform\n",
    "import typing as t\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from tqdm.autonotebook import tqdm\n",
    "import safetensors.torch as st\n",
    "\n",
    "import dataset_processing\n",
    "import helpers\n",
    "\n",
    "lg = helpers.logging.get_logger(\"main\")\n",
    "lg.debug(\"Successfully imported packages.\")"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.698859Z",
     "start_time": "2025-03-03T12:59:51.415760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "    lg.debug(f'Found {torch.cuda.get_device_name()} to use as a cuda device.')\n",
    "elif platform.system() == 'Darwin':\n",
    "    torch_device = torch.device('mps')\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "lg.info(f'Using {torch_device} as torch device.')\n",
    "\n",
    "if platform.system() != 'Linux':\n",
    "    torch.set_num_threads(1)\n",
    "    lg.debug('Set number of threads to 1 as using a non-Linux machine.')"
   ],
   "id": "73aa079af9c51c39",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.893469Z",
     "start_time": "2025-03-03T12:59:51.888666Z"
    }
   },
   "cell_type": "code",
   "source": "random_seed = 42  # todo: turn these into command line args",
   "id": "8fda90a4135632c9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.924089Z",
     "start_time": "2025-03-03T12:59:51.915372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np_rng = np.random.default_rng(random_seed)\n",
    "_ = torch.manual_seed(random_seed)\n",
    "lg.debug(f'Random seed set to {random_seed}.')"
   ],
   "id": "f0606af0834bddb2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.941757Z",
     "start_time": "2025-03-03T12:59:51.938818Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoints_root_name = \"checkpoints_temp\"  # todo: remove temp",
   "id": "d6691dcfddbb4446",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.964766Z",
     "start_time": "2025-03-03T12:59:51.960270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoints_path = Path.home() / \"l3_project\" / checkpoints_root_name\n",
    "checkpoints_path.mkdir(exist_ok=True)\n",
    "lg.debug(f'Checkpoints directory set to {checkpoints_path.resolve()}.')"
   ],
   "id": "596df06089e6e493",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.028292Z",
     "start_time": "2025-03-03T12:59:52.022794Z"
    }
   },
   "cell_type": "code",
   "source": "DATASET_NAMES = t.Literal[\"EuroSATRGB\", \"EuroSATMS\"]",
   "id": "5f199ccb21e0e360",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.051332Z",
     "start_time": "2025-03-03T12:59:52.045786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataset_object(\n",
    "        name: DATASET_NAMES,\n",
    "        split: t.Literal[\"train\", \"val\", \"test\"],\n",
    "        image_size: int,\n",
    "        download: bool = False,\n",
    "        do_transforms: bool = True,\n",
    "):\n",
    "    kwargs = {\n",
    "        \"split\": split,\n",
    "        \"image_size\": image_size,\n",
    "        \"download\": download,\n",
    "        \"do_transforms\": do_transforms,\n",
    "    }\n",
    "\n",
    "    if name == \"EuroSATRGB\":\n",
    "        lg.debug(\"Loading EuroSATRGB dataset...\")\n",
    "        ds = dataset_processing.eurosat.EuroSATRGB(**kwargs)\n",
    "    elif name == \"EuroSATMS\":\n",
    "        lg.debug(\"Loading EuroSATMS dataset...\")\n",
    "        ds = dataset_processing.eurosat.EuroSATMS(**kwargs)\n",
    "    else:\n",
    "        lg.error(f\"Invalid dataset name ({name}) provided to get_dataset_object.\")\n",
    "        raise ValueError(f\"Dataset {name} does not exist.\")\n",
    "\n",
    "    lg.info(f\"Dataset {name} ({split}) loaded with {len(ds)} samples.\")\n",
    "    return ds"
   ],
   "id": "827638db96c60b8f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.079016Z",
     "start_time": "2025-03-03T12:59:52.071728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model_type(\n",
    "        name: t.Literal[\"ResNet50\"],\n",
    ") -> t.Type[helpers.models.FreezableModel]:\n",
    "    if name == \"ResNet50\":\n",
    "        lg.debug(\"Returning ResNet50 model type...\")\n",
    "        m = helpers.models.FineTunedResNet50\n",
    "    else:\n",
    "        lg.error(f\"Invalid model name ({name}) provided to get_model_type.\")\n",
    "        raise ValueError(f\"Model {name} does not exist.\")\n",
    "\n",
    "    return m"
   ],
   "id": "bb96ba54def66f4b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.144805Z",
     "start_time": "2025-03-03T12:59:52.138632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = \"EuroSATMS\"\n",
    "model_name = \"ResNet50\""
   ],
   "id": "41057d24ac7e07ea",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.179878Z",
     "start_time": "2025-03-03T12:59:52.175940Z"
    }
   },
   "cell_type": "code",
   "source": "model_type = get_model_type(model_name)",
   "id": "12e0bb8635b5534d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.411098Z",
     "start_time": "2025-03-03T12:59:52.247950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset = get_dataset_object(dataset_name, \"train\", model_type.expected_input_dim)\n",
    "validation_dataset = get_dataset_object(dataset_name, \"val\", model_type.expected_input_dim)"
   ],
   "id": "e175e10431cff2d5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:53.256497Z",
     "start_time": "2025-03-03T12:59:52.430366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = model_type(\n",
    "    n_input_bands=training_dataset.N_BANDS,\n",
    "    n_output_classes=training_dataset.N_CLASSES\n",
    ").to(torch_device)"
   ],
   "id": "702a183095a34d61",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:11:15.965306Z",
     "start_time": "2025-03-03T16:11:15.960238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "num_workers = 4"
   ],
   "id": "c0ee34d28bb112f3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:14:18.855188Z",
     "start_time": "2025-03-03T13:14:18.846308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "validation_iterator = iter(dataset_processing.core.cycle(validation_dataset))"
   ],
   "id": "145d88995ead400d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine tune first and final layer",
   "id": "754a9557ff9b5408"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:53.367253Z",
     "start_time": "2025-03-03T12:59:53.362065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.freeze_layers(1)  # freeze all but the last layer\n",
    "if model.modified_input_layer:  # unfreeze the input layer if we need to train it too\n",
    "    model.unfreeze_input_layers(model.input_layers_to_train)"
   ],
   "id": "f396f8f9c63a23a3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:04:23.084293Z",
     "start_time": "2025-03-03T13:04:23.077312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "frozen_lr = 0.01\n",
    "optimiser_name = \"SGD\"\n",
    "lr_early_stop_threshold = 0.0001"
   ],
   "id": "a73592e8c21314d0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:04:23.685287Z",
     "start_time": "2025-03-03T13:04:23.671553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_opt_and_scheduler(lr: float):\n",
    "    opt: torch.optim.Optimizer = getattr(torch.optim, optimiser_name)(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr, weight_decay=1e-6, momentum=0.9, nesterov=True,\n",
    "    )\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, factor=np.float_power(10, -1 / 4),  # requires 4 reductions to reduce by factor 10 (*0.1)\n",
    "        patience=5, threshold=0.005\n",
    "    )\n",
    "    return opt, sch"
   ],
   "id": "377efe9e32e6dd32",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:01.285390Z",
     "start_time": "2025-03-03T13:10:01.279711Z"
    }
   },
   "cell_type": "code",
   "source": "wandb_track_run = True",
   "id": "cdb55899073f8f1b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(lr: float, frozen: bool = False):\n",
    "    weights_save_path = checkpoints_path / training_dataset.__class__.__name__ / model.__class__.__name__\n",
    "    if frozen:\n",
    "        weights_save_path /= \"frozen_partial\"\n",
    "    else:\n",
    "        weights_save_path /= \"full\"\n",
    "    lg.debug(f\"Output path set to {weights_save_path.resolve()}.\")\n",
    "\n",
    "    optimiser, scheduler = get_opt_and_scheduler(lr)\n",
    "    lg.debug(f\"Initialised optimiser (lr={lr}) and scheduler.\")\n",
    "\n",
    "    if wandb_track_run:\n",
    "        wandb_run = wandb.init(\n",
    "            save_code=True,\n",
    "            project=\"evaluating_xAI_for_RS\",\n",
    "            name=f\"{dataset_name}_{model_name}{'_frozen' if frozen else ''}\",\n",
    "            notes=\"\",\n",
    "            tags=[dataset_name, model_name, \"frozen\" if frozen else \"full\"],\n",
    "            id=\"\",  # REMEMBER TO CHANGE\n",
    "            resume=\"never\",  # 'allow' to resume a crashed run\n",
    "            config={\n",
    "                \"dataset\": dataset_name,\n",
    "                \"batch_size\": batch_size,\n",
    "\n",
    "                \"model\": model_name,\n",
    "                \"model_repr\": repr(model),\n",
    "                \"training\": {\n",
    "                    \"optimiser\": repr(optimiser),\n",
    "                    \"scheduler\": repr(scheduler),\n",
    "                    \"early_stopping_threshold\": lr_early_stop_threshold,\n",
    "                },\n",
    "\n",
    "                \"wandb_init_time\": time.asctime(),\n",
    "                \"save_path\": str(weights_save_path.resolve()),\n",
    "            }\n",
    "        )\n",
    "        lg.info(f\"Initialised wandb run, id={wandb_run.id}.\")\n",
    "    else:\n",
    "        wandb_run = None\n",
    "\n",
    "    with tqdm(total=50, desc=\"Epochs\") as prog_bar1:\n",
    "        for epoch in range(50):\n",
    "            training_loss_arr = np.zeros(0)\n",
    "            training_acc_arr = np.zeros(0)\n",
    "\n",
    "            with tqdm(total=len(training_dataloader), desc=\"Batches\") as prog_bar2:\n",
    "                lg.debug(f\"Starting{' frozen' if frozen else ''} \"\n",
    "                         f\"{model.__class__.__name__} network training epoch {epoch:03}.\")\n",
    "                for i, data in enumerate(training_dataloader):\n",
    "                    images: torch.Tensor = data[\"image\"]\n",
    "                    labels: torch.Tensor = data[\"label\"]\n",
    "\n",
    "                    loss, acc = helpers.ml.train_step(\n",
    "                        model, images, labels, loss_criterion, optimiser\n",
    "                    )\n",
    "                    training_loss_arr = np.append(training_loss_arr, loss)\n",
    "                    training_acc_arr = np.append(training_acc_arr, acc)\n",
    "\n",
    "                    prog_bar2.update()\n",
    "\n",
    "                    if i > 0 and i % 100 == 0:\n",
    "                        training_mean_loss = training_loss_arr.mean()\n",
    "                        training_mean_acc = training_acc_arr.mean()\n",
    "\n",
    "                        prog_bar2.set_postfix(train_loss=training_mean_loss, train_acc=training_mean_acc)\n",
    "                        lg.debug(str(prog_bar2))\n",
    "\n",
    "                        if wandb_run:\n",
    "                            wandb_run.log({\n",
    "                                \"loss/train\": training_mean_loss,\n",
    "                                \"accuracy/train\": training_mean_acc,\n",
    "                                \"total_steps_trained\": (epoch * len(training_dataloader)) + i,\n",
    "                            })\n",
    "\n",
    "                        training_loss_arr = np.zeros(0)\n",
    "                        training_acc_arr = np.zeros(0)\n",
    "\n",
    "            val_mean_loss, val_mean_acc = helpers.ml.validation_step(\n",
    "                model, loss_criterion, validation_iterator, len(validation_dataloader)\n",
    "            )\n",
    "\n",
    "            scheduler.step(val_mean_loss)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            prog_bar1.update()\n",
    "            prog_bar1.set_postfix(val_loss=val_mean_loss, val_acc=val_mean_acc, lr=current_lr)\n",
    "            lg.info(str(prog_bar1))\n",
    "\n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    \"loss/validation\": val_mean_loss,\n",
    "                    \"accuracy/validation\": val_mean_acc,\n",
    "                    \"learning_rate\": current_lr,\n",
    "                })\n",
    "\n",
    "                if epoch != 0 and epoch % 10 == 0:\n",
    "                    model_save_path = weights_save_path / f\"{wandb_run.id}_epoch{epoch:03}.st\"\n",
    "                    st.save_model(model, model_save_path)\n",
    "                    lg.info(f\"Saved model at epoch {epoch} to {model_save_path}.\")\n",
    "\n",
    "            if current_lr < lr_early_stop_threshold:\n",
    "                lg.info(\n",
    "                    f\"Early stopping on low learning rate {current_lr} (loss plateaued at {val_mean_loss} after lr reductions).\")\n",
    "                break\n",
    "\n",
    "    model_save_path = weights_save_path / f\"{wandb_run.id}_final_{val_mean_acc:.3f}.st\"\n",
    "    st.save_model(model, model_save_path)\n",
    "    lg.info(f\"Saved final model to {model_save_path}.\")\n",
    "\n",
    "    if wandb_run:\n",
    "        wandb_run.summary[\"n_epochs\"] = epoch\n",
    "        wandb_run.finish(0)\n",
    "        lg.info(f\"Finished wandb run, id={wandb_run.id}.\")\n",
    "\n",
    "\n",
    "train_model(frozen_lr, True)"
   ],
   "id": "c6c450652d44b796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.unfreeze_layers()\n",
    "full_lr = 0.001"
   ],
   "id": "f94c12d2f16ff226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_model(full_lr, False)",
   "id": "358b46724638e170"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
