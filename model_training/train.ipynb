{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert to python script after running top to bottom in Jupyter without interactions.",
   "id": "77dd7517f5925b3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:25.781827Z",
     "start_time": "2025-03-03T12:59:25.774617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# for when on NCC to be able to import local packages\n",
    "os.chdir(os.path.expanduser(\"~/l3_project\"))"
   ],
   "id": "348d99628124a29f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "57d01b96a727e4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:17:40.642763Z",
     "start_time": "2025-03-03T13:17:40.630082Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import platform\n",
    "import typing as t\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from tqdm.autonotebook import tqdm\n",
    "import safetensors.torch as st\n",
    "\n",
    "import dataset_processing\n",
    "import helpers\n",
    "\n",
    "lg = helpers.logging.get_logger(\"main\")\n",
    "lg.debug(\"Successfully imported packages.\")"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.698859Z",
     "start_time": "2025-03-03T12:59:51.415760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "    lg.debug(f'Found {torch.cuda.get_device_name()} to use as a cuda device.')\n",
    "elif platform.system() == 'Darwin':\n",
    "    torch_device = torch.device('mps')\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "lg.info(f'Using {torch_device} as torch device.')\n",
    "\n",
    "if platform.system() != 'Linux':\n",
    "    torch.set_num_threads(1)\n",
    "    lg.debug('Set number of threads to 1 as using a non-Linux machine.')"
   ],
   "id": "73aa079af9c51c39",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.893469Z",
     "start_time": "2025-03-03T12:59:51.888666Z"
    }
   },
   "cell_type": "code",
   "source": "random_seed = 42",
   "id": "8fda90a4135632c9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.924089Z",
     "start_time": "2025-03-03T12:59:51.915372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np_rng = np.random.default_rng(random_seed)\n",
    "_ = torch.manual_seed(random_seed)\n",
    "lg.debug(f'Random seed set to {random_seed}.')"
   ],
   "id": "f0606af0834bddb2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.941757Z",
     "start_time": "2025-03-03T12:59:51.938818Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoints_root_name = \"checkpoints_temp\"  # todo: remove temp",
   "id": "d6691dcfddbb4446",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:51.964766Z",
     "start_time": "2025-03-03T12:59:51.960270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoints_path = Path.home() / \"l3_project\" / checkpoints_root_name\n",
    "checkpoints_path.mkdir(exist_ok=True)\n",
    "lg.debug(f'Checkpoints directory set to {checkpoints_path.resolve()}.')"
   ],
   "id": "596df06089e6e493",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.028292Z",
     "start_time": "2025-03-03T12:59:52.022794Z"
    }
   },
   "cell_type": "code",
   "source": "DATASET_NAMES = t.Literal[\"EuroSATRGB\", \"EuroSATMS\"]",
   "id": "5f199ccb21e0e360",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.051332Z",
     "start_time": "2025-03-03T12:59:52.045786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataset_object(\n",
    "        name: DATASET_NAMES,\n",
    "        split: t.Literal[\"train\", \"val\", \"test\"],\n",
    "        image_size: int,\n",
    "        download: bool = False,\n",
    "        do_transforms: bool = True,\n",
    "):\n",
    "    kwargs = {\n",
    "        \"split\": split,\n",
    "        \"image_size\": image_size,\n",
    "        \"download\": download,\n",
    "        \"do_transforms\": do_transforms,\n",
    "    }\n",
    "\n",
    "    if name == \"EuroSATRGB\":\n",
    "        lg.debug(\"Loading EuroSATRGB dataset...\")\n",
    "        ds = dataset_processing.eurosat.EuroSATRGB(**kwargs)\n",
    "    elif name == \"EuroSATMS\":\n",
    "        lg.debug(\"Loading EuroSATMS dataset...\")\n",
    "        ds = dataset_processing.eurosat.EuroSATMS(**kwargs)\n",
    "    else:\n",
    "        lg.error(f\"Invalid dataset name ({name}) provided to get_dataset_object.\")\n",
    "        raise ValueError(f\"Dataset {name} does not exist.\")\n",
    "\n",
    "    lg.info(f\"Dataset {name} ({split}) loaded with {len(ds)} samples.\")\n",
    "    return ds"
   ],
   "id": "827638db96c60b8f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.079016Z",
     "start_time": "2025-03-03T12:59:52.071728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model_type(\n",
    "        name: t.Literal[\"ResNet50\"],\n",
    ") -> t.Type[helpers.models.FreezableModel]:\n",
    "    if name == \"ResNet50\":\n",
    "        lg.debug(\"Returning ResNet50 model type...\")\n",
    "        m = helpers.models.FineTunedResNet50\n",
    "    else:\n",
    "        lg.error(f\"Invalid model name ({name}) provided to get_model_type.\")\n",
    "        raise ValueError(f\"Model {name} does not exist.\")\n",
    "\n",
    "    return m"
   ],
   "id": "bb96ba54def66f4b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.144805Z",
     "start_time": "2025-03-03T12:59:52.138632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = \"EuroSATMS\"\n",
    "model_name = \"ResNet50\""
   ],
   "id": "41057d24ac7e07ea",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.179878Z",
     "start_time": "2025-03-03T12:59:52.175940Z"
    }
   },
   "cell_type": "code",
   "source": "model_type = get_model_type(model_name)",
   "id": "12e0bb8635b5534d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:52.411098Z",
     "start_time": "2025-03-03T12:59:52.247950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset = get_dataset_object(dataset_name, \"train\", model_type.expected_input_dim)\n",
    "validation_dataset = get_dataset_object(dataset_name, \"val\", model_type.expected_input_dim)"
   ],
   "id": "e175e10431cff2d5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:53.256497Z",
     "start_time": "2025-03-03T12:59:52.430366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = model_type(\n",
    "    n_input_bands=training_dataset.N_BANDS,\n",
    "    n_output_classes=training_dataset.N_CLASSES\n",
    ").to(torch_device)"
   ],
   "id": "702a183095a34d61",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:53.279393Z",
     "start_time": "2025-03-03T12:59:53.275060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "num_workers = 4"
   ],
   "id": "c0ee34d28bb112f3",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:14:18.855188Z",
     "start_time": "2025-03-03T13:14:18.846308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "validation_iterator = iter(dataset_processing.core.cycle(validation_dataset))"
   ],
   "id": "145d88995ead400d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine tune first and final layer",
   "id": "754a9557ff9b5408"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T12:59:53.367253Z",
     "start_time": "2025-03-03T12:59:53.362065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.freeze_layers(1)  # freeze all but the last layer\n",
    "# unfreeze the input layer if we need to train it too\n",
    "if model.modified_input_layer:\n",
    "    model.unfreeze_input_layers(model.input_layers_to_train)"
   ],
   "id": "f396f8f9c63a23a3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:04:23.084293Z",
     "start_time": "2025-03-03T13:04:23.077312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "frozen_learning_rate = 0.01\n",
    "optimiser_name = \"SGD\"\n",
    "learning_rate_es_threshold = 0.0001\n",
    "\n",
    "weights_save_path = checkpoints_path / training_dataset.__class__.__name__ / model.__class__.__name__"
   ],
   "id": "a73592e8c21314d0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:04:23.685287Z",
     "start_time": "2025-03-03T13:04:23.671553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimiser: torch.optim.Optimizer = getattr(torch.optim, optimiser_name)(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=frozen_learning_rate,\n",
    "    weight_decay=1e-6, momentum=0.9, nesterov=True,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimiser, factor=np.float_power(10, -1 / 4), patience=5, threshold=0.005\n",
    ")"
   ],
   "id": "377efe9e32e6dd32",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:01.285390Z",
     "start_time": "2025-03-03T13:10:01.279711Z"
    }
   },
   "cell_type": "code",
   "source": "wandb_track_run = True",
   "id": "cdb55899073f8f1b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if wandb_track_run:\n",
    "    run = wandb.init(\n",
    "        save_code=True,\n",
    "        project=\"evaluating_xAI_for_RS\",\n",
    "        name=f\"{dataset_name}_{model_name}_frozen\",\n",
    "        notes=\"\",\n",
    "        tags=[dataset_name, model_name, \"frozen_model\"],\n",
    "        id=\"\",  # REMEMBER TO CHANGE\n",
    "        resume=\"never\",  # 'allow' to resume a crashed run\n",
    "        config={\n",
    "            \"dataset\": dataset_name,\n",
    "            \"batch_size\": batch_size,\n",
    "\n",
    "            \"model\": model_name,\n",
    "            \"model_repr\": repr(model),\n",
    "            \"training\": {\n",
    "                \"optimiser\": repr(optimiser),\n",
    "                \"scheduler\": repr(scheduler),\n",
    "                \"early_stopping_threshold\": learning_rate_es_threshold,\n",
    "            },\n",
    "\n",
    "            \"wandb_init_time\": time.asctime(),\n",
    "            \"save_path\": str(weights_save_path.resolve()),\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    run = None"
   ],
   "id": "c83253c12da67f07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with tqdm(total=50, desc=\"Epochs\") as prog_bar1:\n",
    "    for epoch in range(50):\n",
    "        training_loss_arr = np.zeros(0)\n",
    "        training_acc_arr = np.zeros(0)\n",
    "\n",
    "        with tqdm(desc=\"Batches\", total=len(training_dataloader), leave=True) as prog_bar2:\n",
    "            lg.debug(f\"Starting epoch {epoch:03}...\")\n",
    "            for i, data in enumerate(training_dataloader):\n",
    "                images = data[\"image\"]\n",
    "                labels: torch.Tensor = data[\"label\"]\n",
    "\n",
    "                loss, acc = helpers.ml.train_step(\n",
    "                    model, images, labels, loss_criterion, optimiser\n",
    "                )\n",
    "                training_loss_arr = np.append(training_loss_arr, loss)\n",
    "                training_acc_arr = np.append(training_acc_arr, acc)\n",
    "\n",
    "                prog_bar2.update()\n",
    "                if i > 0 and i % 100 == 0:\n",
    "                    training_mean_loss = training_loss_arr.mean()\n",
    "                    training_mean_acc = training_acc_arr.mean()\n",
    "                    prog_bar2.set_postfix(train_loss=training_mean_loss, train_acc=training_mean_acc)\n",
    "                    lg.debug(str(prog_bar2))\n",
    "                    if run:\n",
    "                        run.log({\n",
    "                            \"loss/train\": training_mean_loss,\n",
    "                            \"accuracy/train\": training_mean_acc,\n",
    "                            \"total_steps_trained\": (epoch * len(training_dataloader)) + i,\n",
    "                        })\n",
    "\n",
    "                    training_loss_arr = np.zeros(0)\n",
    "                    training_acc_arr = np.zeros(0)\n",
    "\n",
    "        val_mean_loss, val_mean_acc = helpers.ml.validation_step(\n",
    "            model, loss_criterion, validation_iterator, len(validation_dataloader)\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_mean_loss)\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        prog_bar1.update()\n",
    "        prog_bar1.set_postfix(val_loss=val_mean_loss, lr=current_lr)\n",
    "        lg.info(str(prog_bar1))\n",
    "\n",
    "        if run:\n",
    "            run.log({\n",
    "                \"loss/validation\": val_mean_loss,\n",
    "                \"accuracy/validation\": val_mean_acc,\n",
    "                \"learning_rate\": current_lr,\n",
    "            })\n",
    "\n",
    "            if epoch != 0 and epoch % 10 == 0:\n",
    "                st.save_model(model,\n",
    "                              weights_save_path / f\"{run.id}_epoch{epoch:03}.st\")\n",
    "\n",
    "        if current_lr < learning_rate_es_threshold:\n",
    "            print(\"Early stop on low learning rate (plateaued after reductions)\")\n",
    "            break\n",
    "\n",
    "st.save_model(model,\n",
    "              weights_save_path / f\"{run.id}_final_{val_mean_acc:.3f}.st\")"
   ],
   "id": "c6c450652d44b796"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
