{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert to python script after running top to bottom in Jupyter without interactions.",
   "id": "1e84b5f16108979b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:11:28.264253Z",
     "start_time": "2025-03-03T18:11:28.256817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# for when on NCC to be able to import local packages\n",
    "os.chdir(os.path.expanduser(\"~/l3_project\"))"
   ],
   "id": "d485c9315b5c44c2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:11:38.596323Z",
     "start_time": "2025-03-03T18:11:28.607546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import platform\n",
    "import typing as t\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from tqdm.autonotebook import tqdm\n",
    "import safetensors.torch as st\n",
    "\n",
    "import dataset_processing\n",
    "import helpers\n",
    "\n",
    "lg = helpers.logging.get_logger(\"main\")\n",
    "lg.debug(\"Successfully imported packages.\")"
   ],
   "id": "e0f15fb5969821a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3667913/2833748380.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:11:38.834277Z",
     "start_time": "2025-03-03T18:11:38.690520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "    lg.debug(f'Found {torch.cuda.get_device_name()} to use as a cuda device.')\n",
    "elif platform.system() == 'Darwin':\n",
    "    torch_device = torch.device('mps')\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "lg.info(f'Using {torch_device} as torch device.')\n",
    "\n",
    "if platform.system() != 'Linux':\n",
    "    torch.set_num_threads(1)\n",
    "    lg.debug('Set number of threads to 1 as using a non-Linux machine.')"
   ],
   "id": "dc7eae448ae3434c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "random_seed = 42  # todo: turn these into command line args",
   "id": "73b3f9cdde278794",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np_rng = np.random.default_rng(random_seed)\n",
    "_ = torch.manual_seed(random_seed)\n",
    "lg.debug(f'Random seed set to {random_seed}.')"
   ],
   "id": "c261dda8b8ed5ee3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "checkpoints_root_name = \"checkpoints\"",
   "id": "9f8eb8d7dccfc3b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoints_path = Path.home() / \"l3_project\" / checkpoints_root_name\n",
    "checkpoints_path.mkdir(exist_ok=True)\n",
    "lg.debug(f'Checkpoints directory set to {checkpoints_path.resolve()}.')"
   ],
   "id": "10ab93a078ba2a79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DATASET_NAMES = t.Literal[\"EuroSATRGB\", \"EuroSATMS\"]",
   "id": "17fd369b7640ab09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_dataset_object(\n",
    "        name: DATASET_NAMES,\n",
    "        split: t.Literal[\"train\", \"val\", \"test\"],\n",
    "        image_size: int,\n",
    "        download: bool = False,\n",
    "        do_transforms: bool = True,\n",
    "):\n",
    "    kwargs = {\n",
    "        \"split\": split,\n",
    "        \"image_size\": image_size,\n",
    "        \"download\": download,\n",
    "        \"do_transforms\": do_transforms,\n",
    "    }\n",
    "\n",
    "    if name == \"EuroSATRGB\":\n",
    "        lg.debug(\"Loading EuroSATRGB dataset...\")\n",
    "        ds = dataset_processing.eurosat.EuroSATRGB(**kwargs)\n",
    "    elif name == \"EuroSATMS\":\n",
    "        lg.debug(\"Loading EuroSATMS dataset...\")\n",
    "        ds = dataset_processing.eurosat.EuroSATMS(**kwargs)\n",
    "    else:\n",
    "        lg.error(f\"Invalid dataset name ({name}) provided to get_dataset_object.\")\n",
    "        raise ValueError(f\"Dataset {name} does not exist.\")\n",
    "\n",
    "    lg.info(f\"Dataset {name} ({split}) loaded with {len(ds)} samples.\")\n",
    "    return ds"
   ],
   "id": "13c131d72a4d51d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_model_type(\n",
    "        name: t.Literal[\"ResNet50\"],\n",
    ") -> t.Type[helpers.models.FreezableModel]:\n",
    "    if name == \"ResNet50\":\n",
    "        lg.debug(\"Returning ResNet50 model type...\")\n",
    "        m = helpers.models.FineTunedResNet50\n",
    "    else:\n",
    "        lg.error(f\"Invalid model name ({name}) provided to get_model_type.\")\n",
    "        raise ValueError(f\"Model {name} does not exist.\")\n",
    "\n",
    "    return m"
   ],
   "id": "d14bd451689ab6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_name = \"EuroSATRGB\"\n",
    "model_name = \"ResNet50\""
   ],
   "id": "f116cbf020fac0b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_type = get_model_type(model_name)",
   "id": "fa131d4b2922fed9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_dataset = get_dataset_object(dataset_name, \"train\", model_type.expected_input_dim)\n",
    "validation_dataset = get_dataset_object(dataset_name, \"val\", model_type.expected_input_dim)"
   ],
   "id": "2a55f5e96e69f544",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model_type(\n",
    "    n_input_bands=training_dataset.N_BANDS,\n",
    "    n_output_classes=training_dataset.N_CLASSES\n",
    ").to(torch_device)"
   ],
   "id": "c19237c126d21b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "num_workers = 4"
   ],
   "id": "92783c2826083b44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "validation_iterator = iter(dataset_processing.core.cycle(validation_dataloader))"
   ],
   "id": "854a47d6ce2050e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine tune first and final layer",
   "id": "9b71953815af8b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.freeze_layers(1)  # freeze all but the last layer\n",
    "if model.modified_input_layer:  # unfreeze the input layer if we need to train it too\n",
    "    model.unfreeze_input_layers(model.input_layers_to_train)"
   ],
   "id": "af6ad421a39cd2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "frozen_lr = 0.01\n",
    "optimiser_name = \"SGD\"\n",
    "lr_early_stop_threshold = 0.0001"
   ],
   "id": "da96e58eaced4e54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_opt_and_scheduler(lr: float, steps_per_order: int = 4):\n",
    "    opt: torch.optim.Optimizer = getattr(torch.optim, optimiser_name)(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr, weight_decay=1e-6, momentum=0.9, nesterov=True,\n",
    "    )\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, factor=np.float_power(10, -1 / steps_per_order),\n",
    "        # requires steps_per_order reductions to reduce by factor 10 (*0.1)\n",
    "        patience=5, threshold=0.005\n",
    "    )\n",
    "    return opt, sch"
   ],
   "id": "e8b478d1e7325181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wandb_track_run = True",
   "id": "f02c04a8a5f02a0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(lr: float, frozen: bool = False, num_epochs: int = 50):\n",
    "    weights_save_path = checkpoints_path / training_dataset.__class__.__name__ / model.__class__.__name__\n",
    "    if frozen:\n",
    "        weights_save_path /= \"frozen_partial\"\n",
    "    else:\n",
    "        weights_save_path /= \"full\"\n",
    "    weights_save_path.mkdir(parents=True, exist_ok=True)\n",
    "    lg.debug(f\"Output path set to {weights_save_path.resolve()}.\")\n",
    "\n",
    "    optimiser, scheduler = get_opt_and_scheduler(lr)\n",
    "    lg.debug(f\"Initialised optimiser (lr={lr}) and scheduler.\")\n",
    "\n",
    "    if wandb_track_run:\n",
    "        wandb_run = wandb.init(\n",
    "            save_code=True,\n",
    "            project=\"evaluating_xAI_for_RS\",\n",
    "            name=f\"{dataset_name}_{model_name}{'_frozen' if frozen else ''}\",\n",
    "            tags=[dataset_name, model_name, \"frozen\" if frozen else \"full\"],\n",
    "            config={\n",
    "                \"dataset\": dataset_name,\n",
    "                \"batch_size\": batch_size,\n",
    "\n",
    "                \"model\": model_name,\n",
    "                \"model_repr\": repr(model),\n",
    "                \"training\": {\n",
    "                    \"optimiser\": repr(optimiser),\n",
    "                    \"scheduler\": {\n",
    "                        \"name\": scheduler.__class__.__name__,\n",
    "                        # todo: these attributes are particular to ReduceLROnPlateau\n",
    "                        \"factor\": scheduler.factor,\n",
    "                        \"patience\": scheduler.patience,\n",
    "                        \"threshold\": scheduler.threshold,\n",
    "                    },\n",
    "                    \"early_stopping_threshold\": lr_early_stop_threshold,\n",
    "                },\n",
    "\n",
    "                \"wandb_init_time\": time.asctime(),\n",
    "                \"save_path\": str(weights_save_path.resolve()),\n",
    "            }\n",
    "        )\n",
    "        lg.info(f\"Initialised wandb run, id={wandb_run.id}.\")\n",
    "    else:\n",
    "        wandb_run = None\n",
    "\n",
    "    with tqdm(total=num_epochs, desc=\"Epochs\") as prog_bar1:\n",
    "        for epoch in range(num_epochs):\n",
    "            training_loss_arr = np.zeros(0)\n",
    "            training_acc_arr = np.zeros(0)\n",
    "\n",
    "            lg.info(str(prog_bar1))\n",
    "            with tqdm(total=len(training_dataloader), desc=\"Batches\", leave=False) as prog_bar2:\n",
    "                for i, data in enumerate(training_dataloader):\n",
    "                    images: torch.Tensor = data[\"image\"]\n",
    "                    labels: torch.Tensor = data[\"label\"]\n",
    "\n",
    "                    loss, acc = helpers.ml.train_step(\n",
    "                        model, images, labels, loss_criterion, optimiser\n",
    "                    )\n",
    "                    training_loss_arr = np.append(training_loss_arr, loss)\n",
    "                    training_acc_arr = np.append(training_acc_arr, acc)\n",
    "\n",
    "                    prog_bar2.update()\n",
    "\n",
    "                    if i > 0 and i % (len(training_dataloader) // 5) == 0:\n",
    "                        training_mean_loss = training_loss_arr.mean()\n",
    "                        training_mean_acc = training_acc_arr.mean()\n",
    "\n",
    "                        prog_bar2.set_postfix(train_loss=training_mean_loss, train_acc=training_mean_acc)\n",
    "                        lg.debug(str(prog_bar2))\n",
    "\n",
    "                        if wandb_run:\n",
    "                            wandb_run.log({\n",
    "                                \"loss/train\": training_mean_loss,\n",
    "                                \"accuracy/train\": training_mean_acc,\n",
    "                                \"total_steps_trained\": (epoch * len(training_dataloader)) + i,\n",
    "                            })\n",
    "\n",
    "                        training_loss_arr = np.zeros(0)\n",
    "                        training_acc_arr = np.zeros(0)\n",
    "\n",
    "            val_mean_loss, val_mean_acc = helpers.ml.validation_step(\n",
    "                model, loss_criterion, validation_iterator, len(validation_dataloader)\n",
    "            )\n",
    "\n",
    "            scheduler.step(val_mean_loss)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            prog_bar1.update()\n",
    "            prog_bar1.set_postfix(val_loss=val_mean_loss, val_acc=val_mean_acc, lr=current_lr)\n",
    "\n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    \"loss/validation\": val_mean_loss,\n",
    "                    \"accuracy/validation\": val_mean_acc,\n",
    "                    \"learning_rate\": current_lr,\n",
    "                })\n",
    "\n",
    "                if epoch != 0 and epoch % 10 == 0:\n",
    "                    model_save_path = weights_save_path / f\"{wandb_run.id}_epoch{epoch:03}.st\"\n",
    "                    st.save_model(model, model_save_path)\n",
    "                    lg.info(f\"Saved model at epoch {epoch} to {model_save_path}.\")\n",
    "\n",
    "            if current_lr < lr_early_stop_threshold:\n",
    "                lg.info(\n",
    "                    f\"Early stopping on low learning rate {current_lr} (loss plateaued at {val_mean_loss} after lr reductions).\")\n",
    "                break\n",
    "\n",
    "    model_save_path = weights_save_path / f\"{wandb_run.id}_final_{val_mean_acc:.3f}.st\"\n",
    "    st.save_model(model, model_save_path)\n",
    "    lg.info(f\"Saved final model to {model_save_path}.\")\n",
    "\n",
    "    if wandb_run:\n",
    "        wandb_run.summary[\"n_epochs\"] = epoch\n",
    "        wandb_run.finish(0)\n",
    "        lg.info(f\"Finished wandb run, id={wandb_run.id}.\")"
   ],
   "id": "b18902174bfe4d7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    train_model(frozen_lr, True, 20)\n",
    "except Exception as e:\n",
    "    lg.exception(\"An exception occurred during model(frozen) training.\", exc_info=e, stack_info=True)"
   ],
   "id": "47918b45e28dd571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lg.info(\"Training full (unfrozen) model...\")\n",
    "model.unfreeze_layers()\n",
    "full_lr = 0.001"
   ],
   "id": "d600ad86590c75d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    train_model(full_lr, False, 50)\n",
    "except Exception as e:\n",
    "    lg.exception(\"An exception occurred during model training.\", exc_info=e, stack_info=True)"
   ],
   "id": "96578c95a6d40ccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "95d404c802e77ead",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
