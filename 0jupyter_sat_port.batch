#!/bin/bash
#SBATCH -N 1                                     ##Request a single node
#SBATCH -c 4                                     ##Request 4 cores
#SBATCH -p ug-gpu-small                          ##Use the research GPU partition if I'm a PhD student for example.
#SBATCH --qos="short"                            ##Use the short QoS to run jobs for up to 2 days
#SBATCH --mem=28G                                ##Request 28Gb of System RAM. This is the most available on this partition
#SBATCH -t 01-18:00:00                           ##Request 2 days for this job, the default is only 30 minutes
##SBATCH --gres=gpu:1                             ##Request 1 GPU, we don't mind what GPU we get given. We could use --gres=gpu:ampere:1 instead to ensure we get an 80Gb A100 for example.
#SBATCH --gres=gpu:1g.10gb:1                     #Temporary sbatch bug workaround
##SBATCH --gres=gpu:ampere:1

##SBATCH --mail-user=jgcw74@durham.ac.uk          ##Replace <username> with your CIS username to receive an email when the job starts
##SBATCH --mail-type=BEGIN

#SBATCH --job-name=sat_port_jupyter
#SBATCH -o jupyterOnPort.log

##Activate conda and environment
eval "$(conda shell.bash hook)"
conda activate sat_project

export DATASET_ROOT=~/datasets

port=36429                                       ##Replace <port_number> with the same port number we selected earlier
/usr/bin/ssh -N -f -R $port:localhost:$port ncc1


##Start the notebook
jupyter notebook --no-browser --port $port
